<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2024-08-20T12:22:55+03:30</updated><id>/feed.xml</id><entry><title type="html">RL | Multi-armed Bandits | A k-armed Bandit Problem</title><link href="/2024/08/20/rl-ch2-multi-armed-bandits.html" rel="alternate" type="text/html" title="RL | Multi-armed Bandits | A k-armed Bandit Problem" /><published>2024-08-20T00:00:00+03:30</published><updated>2024-08-20T00:00:00+03:30</updated><id>/2024/08/20/rl-ch2-multi-armed-bandits</id><content type="html" xml:base="/2024/08/20/rl-ch2-multi-armed-bandits.html"><![CDATA[<h1 id="the-goal-of-these-kind-of-posts">The goal of these kind of posts</h1>
<p>While reading the book ‚ÄúReinforcement Learning: An Introduction‚Äù, written by Sutton, the idea of having important summaries of the chapters came into my mind. This way, I and probabaly my future students could quickly review the main core and methods of RL from an introduction point of view.</p>

<h1 id="rl-vs-other-types-of-learning-methods">RL vs other types of learning methods</h1>
<p>RL uses training information that evaluates the <code class="language-plaintext highlighter-rouge">action</code> taken, rather than <code class="language-plaintext highlighter-rouge">instructs</code> by giving correct actions. Evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken. And in RL, this behavior create the need for active exploration, for a good bahavior.</p>

<h1 id="a-k-armed-bandit-problem">A <code class="language-plaintext highlighter-rouge">k-armed</code> Bandit Problem</h1>
<p>The agent is faced repeatedly with a choice among <code class="language-plaintext highlighter-rouge">k</code> different actions. The agent would receive a numerical reward chosen from a stationary probability distribution that depends on the action taken by the agent. The ultimate objective is maximizing the expected total reward over some time period.</p>

<p>The problem is the original form of the <code class="language-plaintext highlighter-rouge">k-armed bandit problem</code>. There are other names like slot machine, or ‚Äúone-armed bandit problem‚Äù, where there is only one action (or lever) for decision making (Why?!).</p>

<h2 id="value-of-action">Value of action</h2>
<p>Each of the k actions, has an expected reward give that that action is selected, and this is called <code class="language-plaintext highlighter-rouge">value</code> of that action.</p>

\[q_{*}(a)=\mathbb{E}[R(t)|A_{t}=a]\]

<h2 id="greedy-actions">Greedy actions</h2>
<p>If we maintain estimates of the action values, then at any time step there is at least one action whose estimaed value is greatest, which is called <code class="language-plaintext highlighter-rouge">greedy</code> actions.</p>

<h2 id="exploiting-vs-exploring">Exploiting vs Exploring</h2>
<p>When the selected action is greedy, the currect knowledge of velues of the actions are <code class="language-plaintext highlighter-rouge">exploiting</code>, while if a nongreedy action be selected, then the system is <code class="language-plaintext highlighter-rouge">exploring</code>, as the selections made, make the system able to improve the estimate of nongreedy action‚Äôs value. There should be a balance between exploitign and exploring, and it could lead to a conflict. There are many balancing methods, but the issue is not completely solved yet.</p>

<h2 id="comprehensive-description-in-the-reference">Comprehensive description in the reference</h2>
<ul>
  <li>Section 2.1, Reinforcement Learning ‚Äì An Introduction, by Richard Sutton and Andrew Barto</li>
</ul>]]></content><author><name>Mohammad Javad Zallaghi</name></author><category term="rl" /><summary type="html"><![CDATA[The goal of these kind of posts While reading the book ‚ÄúReinforcement Learning: An Introduction‚Äù, written by Sutton, the idea of having important summaries of the chapters came into my mind. This way, I and probabaly my future students could quickly review the main core and methods of RL from an introduction point of view.]]></summary></entry><entry><title type="html">Docker with GPU-access for Robotics</title><link href="/2024/08/07/Docker-with-GPU-access-for-Robotics.html" rel="alternate" type="text/html" title="Docker with GPU-access for Robotics" /><published>2024-08-07T00:00:00+03:30</published><updated>2024-08-07T00:00:00+03:30</updated><id>/2024/08/07/Docker%20with%20GPU%20access%20for%20Robotics</id><content type="html" xml:base="/2024/08/07/Docker-with-GPU-access-for-Robotics.html"><![CDATA[<p>#Roboticists would like to create robots, simulate them, develop their software, then deploy on the hardware.</p>

<p>In hashtag#ROS 2 ecosystem, for simulation, we use hashtag#Gazebo. But you may ask, how to model the robot and import it in Gazebo?</p>

<p>üê¢ For modelling the robot, we use URDF format. It‚Äôs kind of xml file that describe the links, joints, geometry, and collision elements of the robot.</p>

<p>ü§∑‚Äç‚ôÇÔ∏è Should you create the URDF manually? Definitely no ‚Äî doing that manually is not possible. So, what is the solution for creating URDFs and loading the robot in Gazebo? I have the answer if you are intersted in using CAD for creating your robot simulation.</p>

<p>üå± I‚Äôve done it, you can see the images from CAD, to ROS 2 package, and finally the Gazebo simulation (see the attached images).</p>

<p><img src="/assets/images/gpu_inside_docker.png" alt="Access to the GPU inside the docker container" /></p>

<p>This is my instruction for creating the URDF and Gazebo simulation for your robot:</p>

<ol>
  <li>CAD:</li>
</ol>

<p>1.1: Model the parts of your robot using FreeCAD software and its part design workbench.</p>

<p>1.2: Define proper LCS (local coordinate frame) on each part where the joint will be defined later.</p>

<ol>
  <li>URDF and ROS2:</li>
</ol>

<p>2.1: Install FreeCAD.CROSS workbench inside FreeCAD. It‚Äôs a tool for easy URDF generation from your robot.</p>

<p>2.2: Define the links using CROSS. Add created parts of your robot to the links.</p>

<p>2.3: Define the joints using CROSS. Define parent and child links, and the position of the joint.</p>

<p>2.4: Define the robot material in created robot object from CROSS and use the CROSS tool for mass and inertial property calculator of the link.</p>

<p>2.5: Export ROS 2 package of your robot modelled using FreeCAD and CROSS. It will create the URDF, mesh files, and Gazebo and hashtag#Rviz launch files for your robot.</p>

<ol>
  <li>Simulation:</li>
</ol>

<p>3.1: Move the generated ROS 2 package to your ROS2 work space.</p>

<p>3.2: Build the package using colcon build.</p>

<p>3.3: Launch the Gazebo launch file of the package.</p>

<p>3.4: Enjoy your simulation and echo state of the joints of the robot using <code class="language-plaintext highlighter-rouge">ros2 topic echo /joint_states</code></p>

<p>‚úÖ If you are interested to learn this by a YouTube video, give me a comment to create it for you. üòä</p>]]></content><author><name>Mohammad Javad Zallaghi</name></author><category term="docker" /><category term="gpu" /><summary type="html"><![CDATA[#Roboticists would like to create robots, simulate them, develop their software, then deploy on the hardware.]]></summary></entry></feed>