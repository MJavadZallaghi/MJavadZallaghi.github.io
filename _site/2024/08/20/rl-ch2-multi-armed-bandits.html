<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>RL | Multi-armed Bandits | A k-armed Bandit Problem</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link type="application/atom+xml" rel="alternate" href="/feed.xml" />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>RL Multi-armed Bandits A k-armed Bandit Problem</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="RL Multi-armed Bandits A k-armed Bandit Problem" />
<meta name="author" content="Mohammad Javad Zallaghi" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The goal of these kind of posts While reading the book “Reinforcement Learning: An Introduction”, written by Sutton, the idea of having important summaries of the chapters came into my mind. This way, I and probabaly my future students could quickly review the main core and methods of RL from an introduction point of view." />
<meta property="og:description" content="The goal of these kind of posts While reading the book “Reinforcement Learning: An Introduction”, written by Sutton, the idea of having important summaries of the chapters came into my mind. This way, I and probabaly my future students could quickly review the main core and methods of RL from an introduction point of view." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-08-20T00:00:00+03:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="RL Multi-armed Bandits A k-armed Bandit Problem" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Mohammad Javad Zallaghi"},"dateModified":"2024-08-20T00:00:00+03:30","datePublished":"2024-08-20T00:00:00+03:30","description":"The goal of these kind of posts While reading the book “Reinforcement Learning: An Introduction”, written by Sutton, the idea of having important summaries of the chapters came into my mind. This way, I and probabaly my future students could quickly review the main core and methods of RL from an introduction point of view.","headline":"RL Multi-armed Bandits A k-armed Bandit Problem","mainEntityOfPage":{"@type":"WebPage","@id":"/2024/08/20/rl-ch2-multi-armed-bandits.html"},"url":"/2024/08/20/rl-ch2-multi-armed-bandits.html"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <nav class="navbar">
  
    <a href="/" class="nav-link ">Home</a>
  
    <a href="/about.html" class="nav-link ">About</a>
  
    <a href="/projects.html" class="nav-link ">Projects</a>
  
    <a href="/blog.html" class="nav-link ">Blog</a>
  
</nav>

    <article class="post">
    <header class="post-header">
        <h1>RL | Multi-armed Bandits | A k-armed Bandit Problem</h1>
        <p class="post-meta">August 20, 2024 - Mohammad Javad Zallaghi</p>
        
        <div class="post-tags">
            <strong>Tags:</strong>
            
            <a href="/tags/rl" class="post-tag">rl</a>
            
            
        </div>
        
    </header>

    <div class="post-content">
        <h1 id="the-goal-of-these-kind-of-posts">The goal of these kind of posts</h1>
<p>While reading the book “Reinforcement Learning: An Introduction”, written by Sutton, the idea of having important summaries of the chapters came into my mind. This way, I and probabaly my future students could quickly review the main core and methods of RL from an introduction point of view.</p>

<h1 id="rl-vs-other-types-of-learning-methods">RL vs other types of learning methods</h1>
<p>RL uses training information that evaluates the <code class="language-plaintext highlighter-rouge">action</code> taken, rather than <code class="language-plaintext highlighter-rouge">instructs</code> by giving correct actions. Evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken. And in RL, this behavior create the need for active exploration, for a good bahavior.</p>

<h1 id="a-k-armed-bandit-problem">A <code class="language-plaintext highlighter-rouge">k-armed</code> Bandit Problem</h1>
<p>The agent is faced repeatedly with a choice among <code class="language-plaintext highlighter-rouge">k</code> different actions. The agent would receive a numerical reward chosen from a stationary probability distribution that depends on the action taken by the agent. The ultimate objective is maximizing the expected total reward over some time period.</p>

<p>The problem is the original form of the <code class="language-plaintext highlighter-rouge">k-armed bandit problem</code>. There are other names like slot machine, or “one-armed bandit problem”, where there is only one action (or lever) for decision making (Why?!).</p>

<h2 id="value-of-action">Value of action</h2>
<p>Each of the k actions, has an expected reward give that that action is selected, and this is called <code class="language-plaintext highlighter-rouge">value</code> of that action.</p>

\[q_{*}(a)=\mathbb{E}[R(t)|A_{t}=a]\]

<h2 id="greedy-actions">Greedy actions</h2>
<p>If we maintain estimates of the action values, then at any time step there is at least one action whose estimaed value is greatest, which is called <code class="language-plaintext highlighter-rouge">greedy</code> actions.</p>

<h2 id="exploiting-vs-exploring">Exploiting vs Exploring</h2>
<p>When the selected action is greedy, the currect knowledge of velues of the actions are <code class="language-plaintext highlighter-rouge">exploiting</code>, while if a nongreedy action be selected, then the system is <code class="language-plaintext highlighter-rouge">exploring</code>, as the selections made, make the system able to improve the estimate of nongreedy action’s value. There should be a balance between exploitign and exploring, and it could lead to a conflict. There are many balancing methods, but the issue is not completely solved yet.</p>

<h2 id="comprehensive-description-in-the-reference">Comprehensive description in the reference</h2>
<ul>
  <li>Section 2.1, Reinforcement Learning – An Introduction, by Richard Sutton and Andrew Barto</li>
</ul>

    </div>

    <footer class="post-footer">
        <p>&copy; 2024 </p>
    </footer>
</article>

  </body>
</html>